{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and embedding\n",
    "In this notebook, we do\n",
    "1. load toyzero raw window data\n",
    "1. generate masks\n",
    "1. patchify and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-consider:**\n",
    "1. mask generating probably should be combined with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired Toyzero data\n",
    "Since this dataset was originally the test dataset of cycleGAN and used exclusively for test purpose, in order to use it as both train and test data for this supervised training, we set the first $1000$ pair of images for training and the remaining for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_fnames(dirname):\n",
    "    \"\"\"\n",
    "    load image fnames.\n",
    "    If amax_dataset_size is not infinity and is less than all available images,\n",
    "    return a random subset of max_dataset_size image fnames.\n",
    "    \"\"\"\n",
    "    assert Path(dirname).exists(), f\"{dirname} doesn't exist\"\n",
    "    image_fnames = np.array(sorted(list(Path(dirname).glob('*npz'))))\n",
    "    return image_fnames\n",
    "\n",
    "\n",
    "class ToyzeroAlignedDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataroot, *, \n",
    "            max_dataset_size, \n",
    "            serial_batches, \n",
    "            train\n",
    "    ):\n",
    "        super(ToyzeroAlignedDataset, self).__init__()\n",
    "\n",
    "        self.serial_batches = serial_batches\n",
    "\n",
    "        dir_A = Path(dataroot)/f'testA'\n",
    "        dir_B = Path(dataroot)/f'testB'\n",
    "        self.image_fnames_A = load_image_fnames(dir_A)\n",
    "        self.image_fnames_B = load_image_fnames(dir_B)\n",
    "        \n",
    "        if train:\n",
    "            self.image_fnames_A = self.image_fnames_A[:1000]\n",
    "            self.image_fnames_B = self.image_fnames_B[:1000]\n",
    "        else:\n",
    "            self.image_fnames_A = self.image_fnames_A[1000:]\n",
    "            self.image_fnames_B = self.image_fnames_B[1000:]\n",
    "\n",
    "        assert len(self.image_fnames_A) == len(self.image_fnames_B), \"The dataset is not aligned\"\n",
    "        \n",
    "        aligned = True\n",
    "        for fname_A, fname_B in zip(self.image_fnames_A, self.image_fnames_B):\n",
    "            if fname_A.stem != fname_B.stem:\n",
    "                aligned = False\n",
    "                break\n",
    "        if not aligned:\n",
    "            print(\"The dataset is not aligned\")\n",
    "            exit()\n",
    "\n",
    "        self.size = len(self.image_fnames_A)\n",
    "\n",
    "        if max_dataset_size != float('inf') and max_dataset_size < self.size:\n",
    "            indices = np.arange(self.size)\n",
    "            np.random.shuffle(indices)\n",
    "            indices = indices[:max_dataset_size]\n",
    "            self.image_fnames_A = self.image_fnames_A[indices]\n",
    "            self.image_fnames_B = self.image_fnames_B[indices]\n",
    "            self.size = max_dataset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __load(self, image_fname):\n",
    "        image = np.load(image_fname)\n",
    "        image = image[image.files[0]]\n",
    "        image = np.expand_dims(np.float32(image), 0)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.serial_batches:\n",
    "            index = index % self.size\n",
    "        else:\n",
    "            index = np.random.randint(0, self.size - 1)\n",
    "        \n",
    "        image_A = self.__load(self.image_fnames_A[index])\n",
    "        image_B = self.__load(self.image_fnames_B[index])\n",
    "\n",
    "        \n",
    "        return {\n",
    "            'A': image_A, \n",
    "            'B': image_B, \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patchify and depatchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 4, 4])\n",
      "tensor([[[[True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True]],\n",
      "\n",
      "         [[True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True]],\n",
      "\n",
      "         [[True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "def patchify(x, patch_size):\n",
    "    \"\"\"\n",
    "    Cut a batch of images into non-overlapping patches and stack them\n",
    "    Input:\n",
    "        - x (Tensor): The input tensor with shape (batch_size(N), n_channels(C), width(W), height(H));\n",
    "        - patch_size (int): the side length of the patch, which means each patch is of size (patch_size, patch_size);\n",
    "    Output:\n",
    "        Tensor of shape (batch_size * num_patches, n_channel, patch_size, patch_size).\n",
    "        The first 1 through num_patches belongs to the first image \n",
    "        and the num_patches + 1 through 2 * num_patches belongs to the second image, etc.\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    image_size = x.size(-1)\n",
    "    patches = [\n",
    "        x[b, :, i: i + patch_size, j: j + patch_size].unsqueeze(0)\n",
    "        for b in range(batch_size)\n",
    "        for i in range(0, image_size, patch_size)\n",
    "        for j in range(0, image_size, patch_size)\n",
    "    ]\n",
    "    return torch.cat(patches, dim=0)\n",
    "\n",
    "\n",
    "def depatchify(x, batch_size):\n",
    "    \"\"\"\n",
    "    Assemble the patches back to a image.\n",
    "    NOTE: We assume that both the image and patch are square.\n",
    "    Input:\n",
    "        - x (Tensor): The input tensor with shape (batch_size * num_patches, n_channel, patch_size, patch_size)\n",
    "        - batch_size (int): number of FULL images in a batch\n",
    "    \"\"\"\n",
    "    patch_size = x.size(-1)\n",
    "    # Number of patches per image\n",
    "    num_patches = x.size(0) // batch_size\n",
    "    # Number of patches along each edge\n",
    "    num_patches_edge = int(np.sqrt(num_patches))\n",
    "    \n",
    "    patches = x.view(-1, num_patches, *x.shape[1:])\n",
    "    images = []\n",
    "    for I in patches:\n",
    "        rows = []\n",
    "        for i in range(num_patches_edge):\n",
    "            start = i * num_patches_edge\n",
    "            row = torch.cat([I[start + j] for j in range(num_patches_edge)], dim=-1)\n",
    "            rows.append(row)\n",
    "        image = torch.cat(rows, dim=-2)\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        images.append(image)\n",
    "    return torch.cat(images, dim=0)\n",
    "\n",
    "\n",
    "# ================================= Test  ================================= \n",
    "patch_size = 4\n",
    "image_size = 8\n",
    "batch_size = 2\n",
    "\n",
    "x = np.random.randint(0, 10, size=(batch_size, 2, image_size, image_size))\n",
    "x = torch.from_numpy(x)\n",
    "p = patchify(x, patch_size)\n",
    "print(p.shape)\n",
    "y = depatchify(p, batch_size)\n",
    "print(x == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, image_size, patch_size, mode='conv', leading=True, source=True):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - d_model (int): the embedding dimension\n",
    "            - image_size (int): We assume square images and image_size is the side length of the square\n",
    "            - patch_size (int): We assume square patches and patch_size is the side length of the square\n",
    "            - mode (str): choose one from ['plain', 'conv', 'fourier']\n",
    "                - plain: normalization + linear (2 * d_model) + activation + normalization + linear (TO BE IMPLEMENTED)\n",
    "                - conv: convolution\n",
    "                - fourier: 2d-fourier embedding (TO BE IMPLEMENTED)\n",
    "            - leading (bool): Whether there is a leading \"token\" indicating the start of a sequence\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.leading = leading\n",
    "        self.source = source\n",
    "        if not leading:\n",
    "            self.source = False\n",
    "        \n",
    "        if mode == 'conv':\n",
    "            num_patches = (image_size // patch_size) ** 2\n",
    "            \n",
    "            num_layers = self.get_exp(patch_size)\n",
    "            if num_layers == -1:\n",
    "                raise ValueError(f\"patch_size ({patch_size}) is not a power of 2\")\n",
    "                \n",
    "            num_channel_base = 2 ** (num_layers - 1)\n",
    "            assert d_model % num_channel_base == 0, \\\n",
    "                f\"d_model ({d_mode}) is not a multiple of ({num_channel_base})\"\n",
    "            \n",
    "            conv_blocks = []\n",
    "            in_channels = 1\n",
    "            out_channels = d_model // num_channel_base\n",
    "            for i in range(num_layers - 1):\n",
    "                conv_blocks += [\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1), \n",
    "                    nn.BatchNorm2d(out_channels), \n",
    "                    nn.ReLU(),\n",
    "                    # nn.LeakyReLU(negative_slope=.2)\n",
    "                ]\n",
    "                in_channels = out_channels\n",
    "                out_channels *= 2\n",
    "            conv_blocks.append(nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1))\n",
    "            self.layers = nn.ModuleList(conv_blocks)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{mode} embedding is not implemented\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_exp(num):\n",
    "        exp = 0\n",
    "        while num % 2 == 0:\n",
    "            num, exp = num // 2, exp + 1\n",
    "        return -1 if num != 1 else exp       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_patches = (x.size(-1) // self.patch_size) ** 2\n",
    "        \n",
    "        assert all([s == self.image_size for s in x.shape[-2:]]), \\\n",
    "            f\"input image must have size ({self.image_size} x {self.image_size}) but input tensor has shape ({x.shape}).\"\n",
    "        p = patchify(x, self.patch_size)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            p = layer(p)\n",
    "            \n",
    "        p = p.squeeze()\n",
    "        \n",
    "        # ADD ANNOTATION HERE\n",
    "        tokenized_images = []\n",
    "        for start in range(0, p.size(0), num_patches):\n",
    "            tokenized_image = p[start: start + num_patches]\n",
    "            if self.leading:\n",
    "                # Now, the tokenized_image is of shape (seq_len, embedding_length), and\n",
    "                # we need to add one row of ones as the top row.\n",
    "                # The parameter 'pad' is in the order (left, right, top, bottom).\n",
    "                tokenized_image = F.pad(tokenized_image, pad=(0, 0, 1, 0), mode='constant', value=1)\n",
    "#                 if not self.source:\n",
    "#                     tokenized_image = tokenized_image[:-1]\n",
    "                # print(f'padded {tokenized_image}')\n",
    "            tokenized_image = tokenized_image.unsqueeze(0)\n",
    "            tokenized_images.append(tokenized_image)\n",
    "        T = torch.cat(tokenized_images, dim=0)    \n",
    "        return T * np.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Define standard linear + softmax generation step.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, image_size, patch_size, mode='conv', leading=True):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - d_model (int): the embedding dimension\n",
    "            - image_size (int): We assume square images and image_size is the side length of the square\n",
    "            - patch_size (int): We assume square patches and patch_size is the side length of the square\n",
    "            - mode (str): choose one from ['plain', 'conv', 'fourier']\n",
    "                - plain: normalization + linear (2 * d_model) + activation + normalization + linear (TO BE IMPLEMENTED)\n",
    "                - conv: convolution\n",
    "                - fourier: 2d-fourier embedding (TO BE IMPLEMENTED)\n",
    "            - leading (bool): Whether there is a leading \"token\" indicating the start of a sequence\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.leading = leading\n",
    "        if mode == 'conv':\n",
    "            num_patches = (image_size // patch_size) ** 2\n",
    "            \n",
    "            num_layers = self.get_exp(patch_size)\n",
    "            if num_layers == -1:\n",
    "                raise ValueError(f\"patch_size ({patch_size}) is not a power of 2\")\n",
    "                \n",
    "            num_channel_base = 2 ** (num_layers - 1)\n",
    "            assert d_model % num_channel_base == 0, \\\n",
    "                f\"d_model ({d_mode}) is not a multiple of ({num_channel_base})\"\n",
    "            \n",
    "            conv_blocks = []\n",
    "            in_channels = d_model\n",
    "            for i in range(num_layers - 1):\n",
    "                out_channels = in_channels // 2\n",
    "                conv_blocks += [\n",
    "                    nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1), \n",
    "                    nn.BatchNorm2d(out_channels), \n",
    "                    nn.ReLU(),\n",
    "                    # nn.LeakyReLU(negative_slope=.2)\n",
    "                ]\n",
    "                in_channels = out_channels\n",
    "            conv_blocks.append(nn.ConvTranspose2d(in_channels, 1, kernel_size=4, stride=2, padding=1))\n",
    "            self.layers = nn.ModuleList(conv_blocks)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{mode} embedding is not implemented\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_exp(num):\n",
    "        exp = 0\n",
    "        while num % 2 == 0:\n",
    "            num, exp = num // 2, exp + 1\n",
    "        return -1 if num != 1 else exp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - x (Tensor): tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1) Stack the embedded tokens from each sequence \n",
    "        #    and get a tensor of shape (batch_size x seq_len, d_model).\n",
    "        if self.leading:\n",
    "            x = torch.cat([tokenized_image[1:] for tokenized_image in x], dim=0)\n",
    "        else:\n",
    "            x = torch.cat([tokenized_image for tokenized_image in x], dim=0)\n",
    "        # x = torch.cat([tokenized_image for tokenized_image in x], dim=0)\n",
    "        # 2) Add width=1 and height=1 to the end of the tensor and \n",
    "        #    get a tensor of shape (batch_size x seq_len, d_model, 1, 1).\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        # 3) Apply deconvolution, decrease the number of freatures\n",
    "        #    while increasing the image size, so that the output\n",
    "        #    has shape (batch_size x seq_len, 1, patch_size, patch_size).\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        # 4) Depatchify the patches back to images.\n",
    "        #    The seq_len is the number of patches, which is a square.\n",
    "        #    The number of patches along each side, P, is the square root of seq_len\n",
    "        #    The resulting tensor is of size \n",
    "        #    (batch_size, 1, patch_size * P, patch_size * P), \n",
    "        #    where patch_size * P is the side length of the original image.\n",
    "        x = depatchify(x, batch_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Embedding and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 512])\n",
      "torch.Size([32, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Set the device, either 'cuda' or 'cpu'\n",
    "device = 'cuda'\n",
    "\n",
    "# Load data\n",
    "dataroot='/sdcc/u/yhuang2/PROJs/GAN/datasets/ls4gan/toyzero_cropped/toyzero_1001_300-128x128_U/'\n",
    "max_dataset_size = 256\n",
    "serial_batches = True\n",
    "# batch_size = max_dataset_size\n",
    "batch_size = 32\n",
    "\n",
    "dataset = ToyzeroAlignedDataset(\n",
    "    dataroot, \n",
    "    max_dataset_size=max_dataset_size, \n",
    "    serial_batches=serial_batches, \n",
    "    # patch_size=16\n",
    "    train=True\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=serial_batches)\n",
    "data = next(iter(loader))\n",
    "\n",
    "# Set parameter\n",
    "image_size = 128\n",
    "patch_size = 16\n",
    "d_model = 512\n",
    "leading = False\n",
    "\n",
    "# Embedding\n",
    "eb = Embedding(d_model, image_size, patch_size, mode='conv', leading=leading).to(device)\n",
    "embedding = eb(data['A'].to(device))\n",
    "print(embedding.shape)\n",
    "\n",
    "# Generator\n",
    "gr = Generator(d_model, image_size, patch_size, mode='conv', leading=leading).to(device)\n",
    "images = gr(embedding)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding-generator network WITHOUT transformer in between\n",
    "As a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(nn.Module):\n",
    "    def __init__(self, embedding, generator):\n",
    "        super(Simple, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return self.generator(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimplePlot(x, out, y, kwargs=None, symlog=True):\n",
    "    \n",
    "    num_samples = 5\n",
    "    cmap = 'bwr'\n",
    "    width = 4\n",
    "    if kwargs:\n",
    "        if 'num_samples' in kwargs:\n",
    "            num_samples = kwargs['num_samples']\n",
    "        if 'cmap' in kwargs:\n",
    "            cmap = kwargs['cmap']\n",
    "        if 'width' in kwargs:\n",
    "            width = kwargs['width']\n",
    "            \n",
    "    print(num_samples)\n",
    "    total_samples = x.size(0)\n",
    "    indices = np.random.choice(total_samples, num_samples, replace=False)\n",
    "    images = torch.stack([x[indices], out[indices], y[indices]]).squeeze().transpose_(0, 1).detach().cpu().numpy()\n",
    "    \n",
    "    width = 4\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(3 * width * 1.1, num_samples * width))\n",
    "    for i, row in enumerate(axes):\n",
    "        for j, ax in enumerate(row):\n",
    "            ax.set_aspect(1)\n",
    "            image = images[i][j]\n",
    "            \n",
    "            if symlog:\n",
    "                im = ax.pcolormesh(\n",
    "                    images[i][j], \n",
    "                    cmap=cmap, \n",
    "                    norm=mcolors.SymLogNorm(\n",
    "                        linthresh=1, \n",
    "                        linscale=1,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                vmin, vmax = image.min(), image.max()\n",
    "                if vmin == 0:\n",
    "                    vmin = -.05\n",
    "                if vmax == 0:\n",
    "                    vmax = .05\n",
    "                divnorm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0., vmax=vmax)\n",
    "                im = ax.pcolormesh(images[i][j], cmap=cmap, norm=divnorm)\n",
    "                \n",
    "            # Color bar\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.1)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # block=False for immediate plotting. \n",
    "    # Otherwise, the plots will show all at once at the end.\n",
    "    plt.show(block=False) \n",
    "    \n",
    "# # ========================== Test ==========================\n",
    "# x = torch.randn(10, 1, 128, 128)\n",
    "# out = torch.randn(10, 1, 128, 128)\n",
    "# y = torch.randn(10, 1, 128, 128)\n",
    "# plot(x, out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(batches, model, loss_compute, logging_freq=50, plot_func=None, plot_kwargs=None):\n",
    "    \"\"\"\n",
    "    Standard training and logging function\n",
    "    \"\"\"    \n",
    "    start = time.time()\n",
    "    total_loss = 0 # cumulative loss\n",
    "    total_image = 0\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "        x = batch['A'].to(device)\n",
    "        out = model(x)\n",
    "        y = batch['B'].to(device)\n",
    "        loss = loss_compute(out, y)\n",
    "        num_images = batch['A'].size(0)\n",
    "        total_loss += loss * batch['A'].size(0)\n",
    "        total_image += num_images\n",
    "\n",
    "        if logging_freq > 0:\n",
    "            if i % logging_freq == logging_freq - 1:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'\\tIteration: {i + 1}')\n",
    "                print(f'\\t\\tCurrent loss per image:\\t{loss / num_images:.6f}')\n",
    "                print(f'\\t\\tAvg. time per batch:\\t{elapsed / logging_freq:.6f}')\n",
    "                start = time.time()\n",
    "    if plot_func:\n",
    "        if plot_kwargs:\n",
    "            plot_func(x, out, y, plot_kwargs)\n",
    "        else:\n",
    "            plot_func(x, out, y) \n",
    "            \n",
    "    return total_loss / total_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"\"\"\n",
    "    Optim wrapper that implements learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "        self._a = self.factor * (self.model_size ** -.5)\n",
    "        self._b = self.warmup ** (-1.5)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        update parameters and rate\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        \"\"\"\n",
    "        Implement lr as defined above.\n",
    "        \"\"\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self._a * min(step ** (-.5), step * self._b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "class LossCompute:\n",
    "    \"\"\"\n",
    "    A simple loss compute and train function.\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        loss = self.criterion(x, y)\n",
    "        \n",
    "        if self.opt is not None:\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter\n",
    "dataroot='/sdcc/u/yhuang2/PROJs/GAN/datasets/ls4gan/toyzero_cropped/toyzero_1001_300-128x128_U/'\n",
    "# Set the device, either 'cuda' or 'cpu'\n",
    "device = 'cuda'\n",
    "image_size = 128\n",
    "patch_size = 16\n",
    "d_model = 512\n",
    "leading = False\n",
    "\n",
    "# Load train data\n",
    "max_dataset_size = 1000\n",
    "serial_batches = True\n",
    "# batch_size = max_dataset_size\n",
    "batch_size = 32\n",
    "\n",
    "dataset = ToyzeroAlignedDataset(\n",
    "    dataroot, \n",
    "    max_dataset_size=max_dataset_size, \n",
    "    serial_batches=serial_batches, \n",
    "    train=True\n",
    ")\n",
    "loader_train = DataLoader(dataset, batch_size=batch_size, shuffle=serial_batches)\n",
    "\n",
    "# Load valid data\n",
    "max_dataset_size = 100\n",
    "serial_batches = True\n",
    "batch_size = max_dataset_size\n",
    "# batch_size = 32\n",
    "\n",
    "dataset = ToyzeroAlignedDataset(\n",
    "    dataroot, \n",
    "    max_dataset_size=max_dataset_size, \n",
    "    serial_batches=serial_batches, \n",
    "    train=False\n",
    ")\n",
    "loader_valid = DataLoader(dataset, batch_size=batch_size, shuffle=serial_batches)\n",
    "\n",
    "checkpoint_folder = 'checkpoints/simple'\n",
    "if not Path(checkpoint_folder).exists():\n",
    "    Path(checkpoint_folder).mkdir(parents=True)\n",
    "checkpoint_freq = 20\n",
    "\n",
    "plot_freq = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding\n",
    "# eb = Embedding(d_model, image_size, patch_size, mode='conv', leading=leading)\n",
    "# # Generator\n",
    "# gr = Generator(d_model, image_size, patch_size, mode='conv', leading=leading)\n",
    "# model = Simple(eb, gr).to(device)\n",
    "\n",
    "# model_opt = NoamOpt(\n",
    "#     d_model,\n",
    "#     1, # factor\n",
    "#     20, # warmup\n",
    "#     torch.optim.Adam(model.parameters(), lr=0, betas=(.9, .98), eps=1e-9)\n",
    "# )\n",
    "\n",
    "# epochs = 200\n",
    "# for epoch in range(epochs):\n",
    "#     print(f'Epoch {epoch + 1} / {epochs}')\n",
    "#     # tells your model that you are training the model. \n",
    "#     # So effectively layers like dropout, batchnorm etc. \n",
    "#     # which behave different on the train and test procedures \n",
    "#     # know what is going on and hence can behave accordingly.\n",
    "#     model.train()\n",
    "#     start = time.time()\n",
    "#     old_lr = get_lr(model_opt.optimizer)\n",
    "#     loss_train = run_epoch(\n",
    "#         loader_train,\n",
    "#         model,\n",
    "#         LossCompute(nn.L1Loss(), model_opt),\n",
    "#         logging_freq=-1\n",
    "#     )\n",
    "#     new_lr = get_lr(model_opt.optimizer)\n",
    "#     print(f'\\tlr:\\t\\t{old_lr:.6f} -> {new_lr:.6f}')\n",
    "#     print(f'\\ttime per epoch:\\t{time.time() - start:.6f} seconds')\n",
    "#     print(f'\\ttrain loss:\\t{loss_train:.6f}')\n",
    "    \n",
    "#     # Validation:\n",
    "#     if epoch % plot_freq == plot_freq - 1:\n",
    "#         plot_func = SimplePlot\n",
    "#     else:\n",
    "#         plot_func = None\n",
    "    \n",
    "#     loss_valid = run_epoch(\n",
    "#         loader_valid,\n",
    "#         model,\n",
    "#         LossCompute(nn.L1Loss()), # don't use an optimizer and the loss won't be back propagated\n",
    "#         logging_freq=-1, \n",
    "#         plot_func=plot_func,\n",
    "#         plot_kwargs={'num_samples': 3}\n",
    "#     )\n",
    "#     print(f'\\tvalid loss:\\t{loss_valid:.6f}')\n",
    "    \n",
    "#     if epoch % checkpoint_freq == checkpoint_freq - 1:\n",
    "#         print('\\tSaving checkpoint')\n",
    "#         fname = f'model_dict_{epoch + 1}'\n",
    "#         torch.save(model.state_dict(), f'{checkpoint_folder}/{fname}.pt')\n",
    "#         torch.save(model.state_dict(), f'{checkpoint_folder}/model_dict_last.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Adding Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    image_size,\n",
    "    patch_size, *,\n",
    "    N=6,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    h=8,\n",
    "    dropout=.1, \n",
    "    mode='conv',\n",
    "    leading=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Construct a model from hyperparameters\n",
    "    Input:\n",
    "        - image_size (int): images are supposed to be square, and the image_size is the side length.\n",
    "        - patch_size (int): patches are supposed to be square, and the patch_size is the side length.\n",
    "        - N (int): number of encoder and decoder blocks.\n",
    "        - d_model (int): d_model is the embedding length of each token. d_model must be divisible by h (number of heads).\n",
    "        - d_ff (int): hidden units in the hidden layer of the feed-forward block.\n",
    "        - h (int): number of heads in the multiheaded attention.\n",
    "        - dropout (float): dropout rate used in many places in the network.\n",
    "        - mode (string): way of embedding and generator.\n",
    "        - leading (bool): whether to add a leading token to the embedded sequence (sequence of patches in our case).\n",
    "    \"\"\"\n",
    "\n",
    "    C = copy.deepcopy\n",
    "\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    encoder = Encoder(EncoderLayer(d_model, C(attn), C(ff), dropout), N)\n",
    "    decoder = Decoder(DecoderLayer(d_model, C(attn), C(attn), C(ff), dropout), N)\n",
    "    src_embedding = Embedding(d_model, image_size, patch_size, mode=mode, leading=leading, source=True)\n",
    "    tgt_embedding = Embedding(d_model, image_size, patch_size, mode=mode, leading=leading, source=False)\n",
    "    model = EncoderDecoder(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        nn.Sequential(src_embedding, C(position)),\n",
    "        nn.Sequential(tgt_embedding, C(position)),\n",
    "        Generator(d_model, image_size, patch_size, mode=mode)#, leading=leading)\n",
    "    )\n",
    "\n",
    "    # (author of this blog): This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Embedding + Transformer + Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_epoch needs to be modified since the feed forward part also need \n",
    "def run_epoch(batches, model, loss_compute, logging_freq=50, plot_func=None, plot_kwargs=None):\n",
    "    \"\"\"\n",
    "    Standard training and logging function\n",
    "    \"\"\"    \n",
    "    start = time.time()\n",
    "    total_loss = 0 # cumulative loss\n",
    "    total_image = 0\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "        x = batch['A'].to(device)\n",
    "        y = batch['B'].to(device)\n",
    "        out = model(x, y) # run_epoch needs to be modified since the feed forward part also need \n",
    "        \n",
    "        loss = loss_compute(out, y)\n",
    "        num_images = batch['A'].size(0)\n",
    "        total_loss += loss * batch['A'].size(0)\n",
    "        total_image += num_images\n",
    "\n",
    "        if logging_freq > 0:\n",
    "            if i % logging_freq == logging_freq - 1:\n",
    "                elapsed = time.time() - start\n",
    "                print(f'\\tIteration: {i + 1}')\n",
    "                print(f'\\t\\tCurrent loss per image:\\t{loss / num_images:.6f}')\n",
    "                print(f'\\t\\tAvg. time per batch:\\t{elapsed / logging_freq:.6f}')\n",
    "                start = time.time()\n",
    "    if plot_func:\n",
    "        if plot_kwargs:\n",
    "            plot_func(x, out, y, plot_kwargs)\n",
    "        else:\n",
    "            plot_func(x, out, y) \n",
    "            \n",
    "    return total_loss / total_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = 'checkpoints/transformer_full'\n",
    "if not Path(checkpoint_folder).exists():\n",
    "    Path(checkpoint_folder).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 10.25M\n"
     ]
    }
   ],
   "source": [
    "model = make_model(image_size, patch_size, N=3, d_model=256).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'number of parameters = {num_params/1024**2:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_opt = NoamOpt(\n",
    "#     d_model,\n",
    "#     1, # factor\n",
    "#     20, # warmup\n",
    "#     torch.optim.Adam(model.parameters(), lr=0, betas=(.9, .98), eps=1e-9)\n",
    "# )\n",
    "\n",
    "# epochs = 200\n",
    "# for epoch in range(epochs):\n",
    "#     print(f'Epoch {epoch + 1} / {epochs}')\n",
    "#     # tells your model that you are training the model. \n",
    "#     # So effectively layers like dropout, batchnorm etc. \n",
    "#     # which behave different on the train and test procedures \n",
    "#     # know what is going on and hence can behave accordingly.\n",
    "#     model.train()\n",
    "#     start = time.time()\n",
    "#     old_lr = get_lr(model_opt.optimizer)\n",
    "#     loss_train = run_epoch(\n",
    "#         loader_train,\n",
    "#         model,\n",
    "#         LossCompute(nn.L1Loss(), model_opt),\n",
    "#         logging_freq=-1\n",
    "#     )\n",
    "#     new_lr = get_lr(model_opt.optimizer)\n",
    "#     print(f'\\tlr:\\t\\t{old_lr:.6f} -> {new_lr:.6f}')\n",
    "#     print(f'\\ttime per epoch:\\t{time.time() - start:.6f} seconds')\n",
    "#     print(f'\\ttrain loss:\\t{loss_train:.6f}')\n",
    "    \n",
    "#     # Validation:\n",
    "#     if epoch % plot_freq == plot_freq - 1:\n",
    "#         plot_func = SimplePlot\n",
    "#     else:\n",
    "#         plot_func = None\n",
    "    \n",
    "#     loss_valid = run_epoch(\n",
    "#         loader_valid,\n",
    "#         model,\n",
    "#         LossCompute(nn.L1Loss()), # don't use an optimizer and the loss won't be back propagated\n",
    "#         logging_freq=-1, \n",
    "#         plot_func=plot_func,\n",
    "#         plot_kwargs={'num_samples': 3}\n",
    "#     )\n",
    "#     print(f'\\tvalid loss:\\t{loss_valid:.6f}')\n",
    "    \n",
    "#     if epoch % checkpoint_freq == checkpoint_freq - 1:\n",
    "#         print('\\tSaving checkpoint')\n",
    "#         fname = f'model_dict_{epoch + 1}'\n",
    "#         torch.save(model.state_dict(), f'{checkpoint_folder}/{fname}.pt')\n",
    "#         torch.save(model.state_dict(), f'{checkpoint_folder}/model_dict_last.pt')\n",
    "        \n",
    "#     memory_cuda = torch.cuda.max_memory_allocated(device='cuda')\n",
    "#     print(f'\\tpeak memory use:\\t{memory_cuda/1024 ** 3:.3f}G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = next(iter(loader_valid))\n",
    "# x = data['A'].to(device)\n",
    "# y = data['B'].to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load('checkpoints/transformer_full/model_dict_last.pt'))\n",
    "# model_str = model.eval()\n",
    "# out = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimplePlot(x, out, y, {'num_samples': 10, 'width': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, x, device):\n",
    "    model.infer()\n",
    "    \n",
    "    memory = model.encode(x)\n",
    "    print(memory.shape)\n",
    "    batch_size = memory.size(0)\n",
    "    d_model = memory.size(-1)\n",
    "    num_patches = memory.size(1) - 1\n",
    "    \n",
    "    ys = torch.ones(batch_size, 1, d_model).to(device)\n",
    "    for i in range(num_patches):\n",
    "        out = model.decode(memory, ys)\n",
    "        ys = torch.cat([ys, out], dim=1)\n",
    "    \n",
    "    return model.generator(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "input image must have size (128 x 128) but input tensor has shape (torch.Size([2, 2, 8, 8])).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8af680d87cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-4af4888bf988>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(model, x, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/direct/sdcc+u/yhuang2/PROJs/GAN/supervisedTransformer/transformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-ab54bcf25c2f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;34mf\"input image must have size ({self.image_size} x {self.image_size}) but input tensor has shape ({x.shape}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: input image must have size (128 x 128) but input tensor has shape (torch.Size([2, 2, 8, 8]))."
     ]
    }
   ],
   "source": [
    "greedy_decode(model, x, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yi_test",
   "language": "python",
   "name": "yi_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
